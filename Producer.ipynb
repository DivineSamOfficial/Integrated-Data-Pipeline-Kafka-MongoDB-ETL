{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50492d62-f0c2-413a-a4e1-a361f81d1e0c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Library Imports  (Packages to be installed - pymongo, confluent-kafka)\n",
    "This code snippet is written in Python and it involves importing necessary modules for working with MongoDB and Kafka.\n",
    "\n",
    "1. `from pymongo import MongoClient`: This line imports the `MongoClient` class from the `pymongo` module. The `MongoClient` class is used to establish a connection to a MongoDB database and perform operations on it.\n",
    "\n",
    "2. `from confluent_kafka import Producer`: This line imports the `Producer` class from the `confluent_kafka` module. The `Producer` class is used to produce messages to a Kafka topic.\n",
    "\n",
    "3. `import json`: This line imports the built-in `json` module in Python, which provides functions for encoding and decoding JSON data. This module is commonly used for working with JSON data in Python.\n",
    "\n",
    "By importing these modules, you can now use the `MongoClient` class to interact with MongoDB databases and the `Producer` class to produce messages to Kafka topics in your Python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f02273e-1052-403e-9247-8c8d088562de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from confluent_kafka import Producer\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a08f7561-7ecb-4570-a59e-997b9945bd3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Configurations\n",
    "Here we are establishing a connection to a MongoDB database and accessing a specific collection within that database using the `pymongo` module in Python.\n",
    "\n",
    "1. `mongo_client = MongoClient('mongodb+srv://divinesam100:[your_password]@cluster0.daoynj5.mongodb.net/')`: This line creates a `MongoClient` object named `mongo_client` that connects to a MongoDB database hosted on the server `cluster0.daoynj5.mongodb.net`. The connection string includes the username `divinesam100` and a placeholder `[your_password]` for the password. You need to replace `[your_password]` with the actual password to authenticate and establish a connection to the MongoDB database.\n",
    "\n",
    "2. `mongo_db = mongo_client['student_db']`: This line selects the database named `student_db` from the MongoDB server and assigns it to the variable `mongo_db`. This allows us to work with the collections within the `student_db` database.\n",
    "\n",
    "3. `mongo_collection = mongo_db['info']`: This line selects a specific collection named `info` from the `student_db` database and assigns it to the variable `mongo_collection`. A collection in MongoDB is similar to a table in a relational database and stores documents (data records) in a structured format.\n",
    "\n",
    "By executing these lines of code with the correct password provided in the connection string, you can establish a connection to the MongoDB database, select the `student_db` database, and access the `info` collection within that database for performing operations like inserting, updating, querying, or deleting documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9afaf3b-3ed7-482f-923b-91440e0b4483",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mongo_client = MongoClient('mongodb+srv://divinesam100:Divinesam1..@cluster0.daoynj5.mongodb.net/')\n",
    "mongo_db = mongo_client['student_db']\n",
    "mongo_collection = mongo_db['info']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "992c8d5c-4576-4e2a-87d0-fbc3353354a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We are defining a dictionary named `kafka_conf` that contains configuration settings for connecting to a Kafka cluster using the `confluent_kafka` Python library.\n",
    "\n",
    "1. `'bootstrap.servers': 'pkc-p11xm.us-east-1.aws.confluent.cloud:9092'`: This setting specifies the list of Kafka brokers (bootstrap servers) that the producer or consumer should connect to. In this case, the Kafka cluster is hosted at `pkc-p11xm.us-east-1.aws.confluent.cloud` on port `9092`.\n",
    "\n",
    "2. `'security.protocol': 'SASL_SSL'`: This setting defines the security protocol to be used for communication with the Kafka cluster. In this case, it is set to `SASL_SSL`, which provides secure communication using SASL (Simple Authentication and Security Layer) over SSL (Secure Sockets Layer).\n",
    "\n",
    "3. `'sasl.mechanisms': 'PLAIN'`: This setting specifies the SASL mechanism to be used for authentication. Here, it is set to `PLAIN`, which is a simple username/password authentication mechanism.\n",
    "\n",
    "4. `'sasl.username': 'F6EYWWXMDPDQSNBE'`: This setting provides the username required for SASL authentication when connecting to the Kafka cluster.\n",
    "\n",
    "5. `'sasl.password': 'qis/bvd/QNa6WLOQ6oCM5TNnGMsudIg2GulTtW4SM8QAo7t+j+lHdnFeCv0Z0wU3'`: This setting provides the password required for SASL authentication when connecting to the Kafka cluster. It is important to keep this password secure and not expose it in your code or any public repositories.\n",
    "\n",
    "By setting up these configuration parameters in the `kafka_conf` dictionary, you can establish a secure connection to the specified Kafka cluster using the provided SASL credentials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fc4dae5-5ea6-4fb2-a11e-14ef72c3cb77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "kafka_conf = {'bootstrap.servers': 'pkc-p11xm.us-east-1.aws.confluent.cloud:9092',\n",
    "              'security.protocol': 'SASL_SSL',\n",
    "              'sasl.mechanisms': 'PLAIN',\n",
    "              'sasl.username': 'F6EYWWXMDPDQSNBE',\n",
    "              'sasl.password': 'qis/bvd/QNa6WLOQ6oCM5TNnGMsudIg2GulTtW4SM8QAo7t+j+lHdnFeCv0Z0wU3'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78e6e828-c4a8-4390-a0a7-7a80f5238330",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Kafka producer\n",
    "producer = Producer(kafka_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a27c422b-02f1-4e94-b94e-7d0548b0a63c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Kafka topic\n",
    "topic = 'raw_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "941738ee-d3ed-4901-8cb0-98cb77710a82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Produce Messages to the Kafka Topic\n",
    "In this code snippet, a try-except block is used to handle the process of retrieving data from a MongoDB collection and producing messages to a Kafka topic. Here's an explanation of each part:\n",
    "\n",
    "1. `try:`: The code within this block is executed, and any exceptions that occur during the execution are caught and handled in the `except` block.\n",
    "\n",
    "2. `mongodb_data = mongo_collection.find()`: This line retrieves data from the MongoDB collection stored in the `mongo_collection` variable. The `find()` method is used to fetch all documents from the collection.\n",
    "\n",
    "3. `for data in mongodb_data:`: This loop iterates over each document retrieved from the MongoDB collection.\n",
    "\n",
    "4. `message = json.dumps(data)`: Each document is converted to a JSON string using the `json.dumps()` method to prepare it for sending as a message to the Kafka topic.\n",
    "\n",
    "5. `producer.produce(topic, message.encode('utf-8'))`: The `produce()` method of the Kafka producer is used to send the JSON-encoded message to the specified Kafka topic. The message is encoded to UTF-8 format before sending.\n",
    "\n",
    "6. `producer.flush()`: The `flush()` method is called on the Kafka producer to ensure that all produced messages are sent to the Kafka cluster.\n",
    "\n",
    "7. `print(\"Messages produced successfully.\")`: If the messages are successfully produced to the Kafka topic without any exceptions, this message is printed to the console.\n",
    "\n",
    "8. `except Exception as e:`: If an exception occurs during the try block execution, the code jumps to this block to handle the exception.\n",
    "\n",
    "9. `print(f\"Error producing messages: {e}\")`: If an exception occurs, this line prints an error message indicating that there was an issue producing messages to the Kafka topic, along with the specific exception message (`e`).\n",
    "\n",
    "Overall, this code snippet demonstrates the process of fetching data from a MongoDB collection, converting it to JSON, and then producing the JSON data as messages to a Kafka topic. Any errors encountered during this process are caught and handled appropriately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0519c8d2-d255-4db1-86e8-f4bb66d6ce03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages produced successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Retrieve data from MongoDB collection\n",
    "    mongodb_data = mongo_collection.find()\n",
    "\n",
    "    # Produce messages to the Kafka topic\n",
    "    for data in mongodb_data:\n",
    "        message = json.dumps(data)\n",
    "        producer.produce(topic, message.encode('utf-8'))\n",
    "\n",
    "    # Flush the producer to ensure all messages are sent\n",
    "    producer.flush()\n",
    "\n",
    "    print(\"Messages produced successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error producing messages: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Producer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
