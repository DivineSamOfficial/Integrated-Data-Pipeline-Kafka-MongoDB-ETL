{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90c80744-3eef-4003-8a40-59f1876b5281",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Module Imports\n",
    "1. `import pyspark.sql.functions as F`: This imports the PySpark SQL functions module and aliases it as `F`. This module contains many built-in functions for data manipulation and \n",
    "    transformation in PySpark.\n",
    "\n",
    "2. `from pyspark.sql.functions import col`, `struct, to_json`: This imports specific functions `col`, `struct`, and `to_json` from the `pyspark.sql.functions` module.\n",
    "\n",
    "3. `col` is used to refer to a column in a `DataFrame` by its name.\n",
    "4. `struct` is used to combine multiple columns into a `struct` column, which is a way to group related data together.\n",
    "5. `to_json` is used to convert a `DataFrame` column or a `struct` column into a `JSON` string.\n",
    "6. `from pyspark.sql.types import *`: This imports all types from the `pyspark.sql.types module`. \n",
    "    This module contains data types used in PySpark DataFrame schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aff96f07-e7b3-4a36-8aea-d031650b6290",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from  pyspark.sql.functions import col, struct, to_json, when\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07d62a48-6032-4e7c-b8c0-4842856e5dd8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Reading Data from KAFKA topic `raw_data` into a `Dataframe`\n",
    "1. `df_read`: This variable stores the `DataFrame` created by reading data from a Kafka topic using PySpark.\n",
    "\n",
    "2. `.format(\"kafka\")`: Specifies that the data source format is Kafka.\n",
    "\n",
    "3. `.option(\"kafka.bootstrap.servers\", \"pkc-p11xm.us-east-1.aws.confluent.cloud:9092\")`: Sets the Kafka bootstrap servers.\n",
    "\n",
    "4. `.option(\"subscribe\", \"raw_data\")`: Specifies the topic to subscribe to (\"raw_data\" in this case).\n",
    "\n",
    "5. `.option(\"startingOffsets\", \"earliest\")`: Sets the starting offset for reading messages from the topic to the earliest available offset.\n",
    "\n",
    "6. `.option(\"endingOffsets\", \"latest\")`: Sets the ending offset for reading messages from the topic to the latest available offset.\n",
    "\n",
    "7. `.option(\"kafka.security.protocol\",\"SASL_SSL\")`: Sets the security protocol to `SASL_SSL` for secure communication.\n",
    "\n",
    "8. `.option(\"kafka.sasl.mechanism\", \"PLAIN\")`: Specifies the `SASL` mechanism as PLAIN.\n",
    "\n",
    "9. `.option(\"kafka.sasl.jaas.config\", \"...\")`: Sets the JAAS configuration for `SASL` authentication with the provided username and password.\n",
    "\n",
    "10. `.load()`: Loads the data from Kafka into the `DataFrame`.\n",
    "\n",
    "11. `display(df_read)`: Displays the `DataFrame` in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06052f0b-e993-4ac4-8ae7-3b46383b9222",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_read = spark \\\n",
    "      .read \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"pkc-p11xm.us-east-1.aws.confluent.cloud:9092\") \\\n",
    "      .option(\"subscribe\", \"raw_data\") \\\n",
    "      .option(\"startingOffsets\", \"earliest\") \\\n",
    "      .option(\"endingOffsets\", \"latest\")  \\\n",
    "      .option(\"kafka.security.protocol\",\"SASL_SSL\") \\\n",
    "      .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "      .option(\"kafka.sasl.jaas.config\", \"\"\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"F6EYWWXMDPDQSNBE\" password=\"qis/bvd/QNa6WLOQ6oCM5TNnGMsudIg2GulTtW4SM8QAo7t+j+lHdnFeCv0Z0wU3\";\"\"\") \\\n",
    "      .load()\n",
    "display(df_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44cc49f5-7079-4d8e-ab3f-2b3a1f73267d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5d7eb4d-1038-40f2-9185-9ccd3f5251b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Schema Defintition\n",
    "1. `from pyspark.sql.types` import `StructType`, `StructField`, `IntegerType`, `StringType`: Imports necessary classes and data types from the `pyspark.sql.types` module.\n",
    "2. `json_schema` = `StructType([...]`): Defines a JSON schema using `StructType`, which represents a `schema` for structured data, like `JSON` or `CSV`.\n",
    "3. `StructField(\"_id\", IntegerType(), True)`: Defines a field named `\"_id\"` with an integer data type `(IntegerType())` that allows `null values` `(True)`.\n",
    "4. `StructField(\"name\", StringType(), True)`: Defines a field named `\"name\"` with a `string data type (StringType())` that allows null values `(True)`.\n",
    "5. Similarly, other fields like `\"age\", \"grade\", \"attendance\", and \"marks_outof350\"` are defined with their respective data types `(IntegerType() or StringType())` and `nullability`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "678d2fba-07ba-4918-9108-9f416007c67c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_schema = StructType(\n",
    "    [   StructField(\"_id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"age\", IntegerType(), True),\n",
    "        StructField(\"grade\", StringType(), True),\n",
    "        StructField(\"attendance\", IntegerType(), True),\n",
    "        StructField(\"marks_outof350\", IntegerType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2491956-69b4-46f4-bee7-479e92a88054",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read the Data from a Kafka topic\n",
    "In this code snippet, a DataFrame `df_read` is being transformed using PySpark operations to extract specific fields from a JSON column and create a new DataFrame `df` with the selected fields. Here's an explanation of each part:\n",
    "\n",
    "1. `df = df_read.withColumn('value', F.from_json(F.col('value').cast('string'), json_schema))`: This line adds a new column 'value' to the DataFrame `df_read`. The `from_json()` function is used to parse the JSON string in the 'value' column (after casting it to a string) based on the specified `json_schema`. The result is a DataFrame with an additional 'value' column containing the parsed JSON data.\n",
    "\n",
    "2. `.select(`: This method is used to select specific columns from the DataFrame.\n",
    "\n",
    "3. `F.col(\"value._id\").alias(\"student_id\")`: This selects the '_id' field from the 'value' column and renames it as 'student_id'.\n",
    "\n",
    "4. `F.col(\"value.name\")`: This selects the 'name' field from the 'value' column.\n",
    "\n",
    "5. `F.col(\"value.age\")`: This selects the 'age' field from the 'value' column.\n",
    "\n",
    "6. `F.col(\"value.grade\")`: This selects the 'grade' field from the 'value' column.\n",
    "\n",
    "7. `F.col(\"value.attendance\")`: This selects the 'attendance' field from the 'value' column.\n",
    "\n",
    "8. `F.col(\"value.marks_outof350\")`: This selects the 'marks_outof350' field from the 'value' column.\n",
    "\n",
    "9. `)`: Closing bracket for the `select()` method.\n",
    "\n",
    "10. `display(df)`: Finally, the `display()` function is used to show the DataFrame `df` in a tabular format. This function is typically used in Databricks notebooks or similar environments to visualize the DataFrame content.\n",
    "\n",
    "In summary, this code snippet processes a DataFrame by parsing a JSON column, selecting specific fields from the parsed JSON data, and displaying the resulting DataFrame `df` with the selected fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a2c8cfd-8013-46f3-9d54-278392bf7618",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = df_read.withColumn('value', F.from_json(F.col('value').cast('string'), json_schema))  \\\n",
    "      .select(\n",
    "                  F.col(\"value._id\").alias(\"student_id\"),\n",
    "                  F.col(\"value.name\"),\n",
    "                  F.col(\"value.age\"),\n",
    "                  F.col(\"value.grade\"),\n",
    "                  F.col(\"value.attendance\"),\n",
    "                  F.col(\"value.marks_outof350\")) \n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb109e05-6b14-4a17-b1af-a050f333b9b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Saving to DBFS as a Delta Table\n",
    "delta_path = \"dbfs:/FileStore/tables\"\n",
    "df_studentinfo_delta = df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"student_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce51f9b5-6c55-4471-b21f-90f18f101dc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_student_delta = spark.table('student_delta')\n",
    "df_student_delta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fd67734-5c06-42d7-8c9d-518424dc9792",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Transformations\n",
    "1. Drop duplicates in `df_student_delta`.\n",
    "2. Fill null values in the \"Marks_outof350\" column:\n",
    "   - If null, multiply \"Attendance\" by 12.\n",
    "   - Otherwise, keep the existing value.\n",
    "3. Capitalize column names in `df_student_delta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f612d5d4-9b7a-4b0e-8830-bd0350f8d407",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_student_delta = df_student_delta.dropDuplicates()\n",
    "\n",
    "df_student_delta = df_student_delta.withColumn(\"marks_outof350\", \n",
    "                                               when(col(\"marks_outof350\").isNull(), col(\"attendance\") * 12)\n",
    "                                               .otherwise(col(\"marks_outof350\")))\n",
    "\n",
    "\n",
    "df_student_delta = df_student_delta.toDF(*(col_name.capitalize() for col_name in df_student_delta.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429b9607-6615-4572-9f9a-8c2fb4cb467c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sorting the table in an ascending order\n",
    "df_student_delta = df_student_delta.orderBy(col(\"student_id\").asc())\n",
    "df_student_delta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a11da63d-eacc-43cc-a394-d8dcbb851c9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Wrting Data into a new Kafka topic\n",
    "\n",
    "\n",
    "\n",
    "  1. Select \"Name\" column as \"key\" and convert to JSON.\n",
    "  2. Write to Kafka topic \"transformed_data\":\n",
    "    - Kafka broker: pkc-p11xm.us-east-1.aws.confluent.cloud:9092\n",
    "    - Security protocol: SASL_SSL\n",
    "    - SASL mechanism: PLAIN\n",
    "    - SASL username: {YOUR_SASL_USERNAME}\n",
    "    - SASL password: {YOUR_SASL_PASSWORD}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e44b10c-9afc-4069-9c6c-2a5cf87e18eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_student_delta.selectExpr(\"name AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "    .write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"pkc-p11xm.us-east-1.aws.confluent.cloud:9092\") \\\n",
    "    .option(\"topic\", \"transformed_data\") \\\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "    .option(\"kafka.sasl.jaas.config\", \"\"\"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"F6EYWWXMDPDQSNBE\" password=\"qis/bvd/QNa6WLOQ6oCM5TNnGMsudIg2GulTtW4SM8QAo7t+j+lHdnFeCv0Z0wU3\";\"\"\") \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Extract-Transform-Load",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
